"""
KcELECTRA ê°ì„± ë¶„ì„ ëª¨ë¸ Fine-tuning (ì˜¤íƒ€/ì‹ ì¡°ì–´ ëŒ€ì‘)
ìµœì‹  transformers ë²„ì „ í˜¸í™˜
"""

import os
import torch
import numpy as np
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import random

# ========================================
# ì„¤ì •
# ========================================

# ê¸°ë³¸ ëª¨ë¸ (HuggingFace)
BASE_MODEL = "beomi/KcELECTRA-base-v2022"

# ê²½ë¡œ ì„¤ì •
MODEL_PATH = "./models/kcelectra_sentiment_finetuned"
AUGMENTED_DATA_PATH = "./data/nsmc/ratings_train_augmented.txt"
TEST_DATA_PATH = "./data/nsmc/ratings_test.txt"
OUTPUT_DIR = "./models/kcelectra_sentiment_with_typo"

# Fine-tuning íŒŒë¼ë¯¸í„°
BATCH_SIZE = 16
EPOCHS = 3
LEARNING_RATE = 2e-5
MAX_LENGTH = 128

# ========================================
# 1. ë°ì´í„° ì¦ê°• (ì˜¤íƒ€/ì‹ ì¡°ì–´ ì¶”ê°€)
# ========================================

def augment_with_typo(text, prob=0.15):
    """ì˜¤íƒ€ ë° ì‹ ì¡°ì–´ ì¦ê°•"""

    slang_dict = {
        "ì¢‹ë‹¤": ["êµ¿", "ì¢‹ìŒ", "ì¡°ìŒ", "ì¡°íƒ€", "ì¡°ì•„"],
        "ìµœê³ ": ["ìµœê³ ì„", "ì©ë‹¤", "ë ˆì „ë“œ"],
        "ì¬ë¯¸ìˆë‹¤": ["ì¬ë°Œë‹¤", "ì¼", "ì¼ìˆìŒ", "ì¼ë”°"],
        "ë³„ë¡œ": ["ë³„ë£¨", "ë…¸ì¼"],
        "ìµœì•…": ["ìµœì•…ì„", "ìµœì•…ì´ë‹¤", "ë ˆì•Œìµœì•…"],
        "ì˜í™”": ["ì˜í™”ì„", "ë¬´ë¹„"],
        "ê°ë™": ["ê°ë™ì„", "ê°ë™ì ", "ê°ë™ì´ì•¼"],
        "ì§„ì§œ": ["ë ˆì•Œ", "ã„¹ã…‡", "ë¦¬ì–¼"],
        "ì •ë§": ["ì§„ã…ì", "ì •ã…ë§"],
    }

    if random.random() < prob:
        for word, slangs in slang_dict.items():
            if word in text:
                text = text.replace(word, random.choice(slangs))

    if random.random() < prob * 0.5:
        words = text.split()
        if len(words) > 1:
            idx = random.randint(0, len(words) - 2)
            words[idx] = words[idx] + words[idx + 1]
            words.pop(idx + 1)
            text = " ".join(words)

    return text

def create_augmented_dataset():
    """ì¦ê°• ë°ì´í„°ì…‹ ìƒì„±"""

    print("=" * 70)
    print("ğŸ“Š ì˜¤íƒ€/ì‹ ì¡°ì–´ ë°ì´í„° ì¦ê°•")
    print("=" * 70)

    data_path = "./data/nsmc/ratings_train.txt"

    if not os.path.exists(data_path):
        print(f"âš ï¸ NSMC ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        print("ğŸ’¡ python download_nsmc.py ì‹¤í–‰í•˜ì„¸ìš”")
        raise FileNotFoundError(f"NSMC ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {data_path}")

    texts = []
    labels = []

    with open(data_path, 'r', encoding='utf-8') as f:
        next(f)
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) == 3:
                _, text, label = parts
                texts.append(text)
                labels.append(int(label))

    print(f"\nì›ë³¸ ë°ì´í„°: {len(texts):,}ê°œ")

    augment_ratio = 0.3
    n_augment = int(len(texts) * augment_ratio)

    augmented_texts = []
    augmented_labels = []

    indices = random.sample(range(len(texts)), n_augment)

    for idx in indices:
        original_text = texts[idx]
        augmented_text = augment_with_typo(original_text, prob=0.2)
        augmented_texts.append(augmented_text)
        augmented_labels.append(labels[idx])

    all_texts = texts + augmented_texts
    all_labels = labels + augmented_labels

    print(f"\nâœ… ì¦ê°• ì™„ë£Œ!")
    print(f"   ì›ë³¸: {len(texts):,}ê°œ")
    print(f"   ì¦ê°•: {len(augmented_texts):,}ê°œ")
    print(f"   ì „ì²´: {len(all_texts):,}ê°œ")

    os.makedirs(os.path.dirname(AUGMENTED_DATA_PATH), exist_ok=True)

    with open(AUGMENTED_DATA_PATH, 'w', encoding='utf-8') as f:
        f.write("id\tdocument\tlabel\n")
        for i, (text, label) in enumerate(zip(all_texts, all_labels)):
            f.write(f"{i}\t{text}\t{label}\n")

    print(f"   ì €ì¥: {AUGMENTED_DATA_PATH}")

    print("\nğŸ“‹ ì¦ê°• ìƒ˜í”Œ:")
    for text, label in zip(augmented_texts[:5], augmented_labels[:5]):
        print(f"  {text} (label={label})")

    return all_texts, all_labels

# ========================================
# 2. ë°ì´í„° ë¡œë”©
# ========================================

def load_local_nsmc(filepath):
    """ë¡œì»¬ NSMC íŒŒì¼ ë¡œë”©"""

    texts = []
    labels = []

    with open(filepath, 'r', encoding='utf-8') as f:
        next(f)
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) == 3:
                _, text, label = parts
                texts.append(text)
                labels.append(int(label))

    return Dataset.from_dict({
        "text": texts,
        "label": labels
    })

def load_data():
    """ì¦ê°• ë°ì´í„° ë¡œë”©"""

    if not os.path.exists(AUGMENTED_DATA_PATH):
        create_augmented_dataset()

    if not os.path.exists(TEST_DATA_PATH):
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {TEST_DATA_PATH}")
        print("ğŸ’¡ python download_nsmc.py ì‹¤í–‰í•˜ì„¸ìš”")
        raise FileNotFoundError(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {TEST_DATA_PATH}")

    print("  í•™ìŠµ ë°ì´í„° ë¡œë”©...")
    train_dataset = load_local_nsmc(AUGMENTED_DATA_PATH)

    print("  í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©...")
    test_dataset = load_local_nsmc(TEST_DATA_PATH)

    print(f"  í•™ìŠµ: {len(train_dataset):,}ê°œ")
    print(f"  í…ŒìŠ¤íŠ¸: {len(test_dataset):,}ê°œ")

    return train_dataset, test_dataset

# ========================================
# 3. ëª¨ë¸ & Tokenizer ë¡œë”©
# ========================================

def load_model_and_tokenizer():
    """ëª¨ë¸ ë¡œë”©"""

    if os.path.exists(MODEL_PATH) and os.path.exists(os.path.join(MODEL_PATH, "config.json")):
        print(f"  ê¸°ì¡´ íŒŒì¸íŠ  ëª¨ë¸ ë¡œë”©: {MODEL_PATH}")
        model_path = MODEL_PATH
    else:
        print(f"  ê¸°ë³¸ ëª¨ë¸ ë¡œë”©: {BASE_MODEL}")
        model_path = BASE_MODEL

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_path,
        num_labels=2
    )

    return tokenizer, model

# ========================================
# 4. Tokenization
# ========================================

def tokenize_dataset(dataset, tokenizer):
    """ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•"""

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=MAX_LENGTH,
            padding=False
        )

    return dataset.map(tokenize_function, batched=True)

# ========================================
# 5. Fine-tuning
# ========================================

def compute_metrics(eval_pred):
    """í‰ê°€ ë©”íŠ¸ë¦­"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='binary'
    )

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

def train():
    """Fine-tuning ì‹¤í–‰"""

    print("=" * 70)
    print("ğŸ”§ ì˜¤íƒ€/ì‹ ì¡°ì–´ ì¶”ê°€ Fine-tuning")
    print("=" * 70)

    # GPU í™•ì¸
    print("\nğŸ–¥ï¸ GPU í™•ì¸:")
    if torch.cuda.is_available():
        print(f"  âœ… GPU ì‚¬ìš© ê°€ëŠ¥")
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  CUDA ë²„ì „: {torch.version.cuda}")
        print(f"  FP16: í™œì„±í™”")
    else:
        print(f"  âš ï¸ GPU ì—†ìŒ - CPUë¡œ í•™ìŠµ (ëŠë¦´ ìˆ˜ ìˆìŒ)")

    print("\n[1/4] ë°ì´í„° ì¦ê°•...")
    if not os.path.exists(AUGMENTED_DATA_PATH):
        create_augmented_dataset()
    else:
        print(f"  ê¸°ì¡´ ì¦ê°• ë°ì´í„° ì‚¬ìš©: {AUGMENTED_DATA_PATH}")

    print("\n[2/4] ì¦ê°• ë°ì´í„° ë¡œë”©...")
    train_dataset, test_dataset = load_data()

    print("\n[3/4] ëª¨ë¸ ë¡œë”©...")
    tokenizer, model = load_model_and_tokenizer()

    print("\n[4/4] Tokenization...")
    train_dataset = tokenize_dataset(train_dataset, tokenizer)
    test_dataset = tokenize_dataset(test_dataset, tokenizer)

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # Training Arguments (GPU ì‚¬ìš©)
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=LEARNING_RATE,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        num_train_epochs=EPOCHS,
        weight_decay=0.01,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        save_total_limit=2,
        logging_steps=100,
        # GPU ì„¤ì •
        fp16=torch.cuda.is_available(),  # GPU ìˆìœ¼ë©´ FP16 ì‚¬ìš© âœ…
        # use_cpu=False,  # CPU ê°•ì œ ì‚¬ìš© ì•ˆ í•¨
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    print("\n" + "=" * 70)
    print("ğŸš€ Fine-tuning ì‹œì‘!")
    print("=" * 70)

    trainer.train()

    print("\n" + "=" * 70)
    print("ğŸ“Š ìµœì¢… í‰ê°€")
    print("=" * 70)

    results = trainer.evaluate()

    for key, value in results.items():
        print(f"  {key}: {value:.4f}")

    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥: {OUTPUT_DIR}")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    print("\nğŸ‰ Fine-tuning ì™„ë£Œ!")

if __name__ == "__main__":
    train()