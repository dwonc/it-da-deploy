"""
ITDA AI Server - FastAPI Main
"""
from dotenv import load_dotenv
import os
import warnings
import logging

# ========================================
# â­ ëª¨ë“  ê²½ê³  ì™„ì „ ì°¨ë‹¨
# ========================================
os.environ['LIGHTGBM_VERBOSITY'] = '-1'
os.environ['PYTHONWARNINGS'] = 'ignore'

# ëª¨ë“  ê²½ê³  í•„í„°
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', message='.*num_leaves.*')

# LightGBM ë¡œê±° ë¹„í™œì„±í™”
logging.getLogger('lightgbm').setLevel(logging.ERROR)

# âœ… .env ë¡œë“œ
load_dotenv()

# í™˜ê²½ë³€ìˆ˜ í™•ì¸
print("ğŸ”§ FastAPI ì„œë²„ ì‹œì‘...")
print(f"ğŸ“ OPENAI_API_KEY: {'ì„¤ì •ë¨' if os.getenv('OPENAI_API_KEY') else 'âŒ ì—†ìŒ'}")
print(f"ğŸ“ SPRING_BOOT_URL: {os.getenv('SPRING_BOOT_URL', 'http://localhost:8080')}")

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from app.api.ai_routes import router as ai_router
from app.api.recommendations import router as recommendations_router
from app.models.model_loader import model_loader
from app.core.logging import logger

# ========================================
# Lifespan Event Handler
# ========================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    logger.info("ğŸš€ ITDA AI Server ì‹œì‘")

    try:
        model_loader.load_all()
        logger.info("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise

    yield

    logger.info("ğŸ‘‹ ITDA AI Server ì¢…ë£Œ")


app = FastAPI(
    title="ITDA AI Server",
    description="ëª¨ì„ ì¶”ì²œ AI ì„œë²„ (SVD, LightGBM Ranker, KcELECTRA)",
    version="2.0.0",
    lifespan=lifespan
)

# ========================================
# CORS ì„¤ì •
# ========================================

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:5173",
        "http://localhost:8080",
        "http://127.0.0.1:3000",
        "http://127.0.0.1:5173",
        "http://127.0.0.1:8080",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========================================
# ë¼ìš°í„° ë“±ë¡
# ========================================

app.include_router(ai_router)
app.include_router(recommendations_router)

@app.get("/")
async def root():
    return {
        "status": "ok",
        "message": "ITDA AI Server is running",
        "version": "2.0.0",
        "models": model_loader.get_status()
    }

@app.get("/api/ai/recommendations/health")
async def health_check():
    return {
        "status": "healthy",
        "models": model_loader.get_status()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )