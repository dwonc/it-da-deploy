"""
LightGBM Ranker v2 - Improved
- ë” ê°•í•œ ì°¨ë³„í™”ë¥¼ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
- Feature importance í™•ì¸
- ì„±ëŠ¥ í‰ê°€ ì¶”ê°€
"""

import pickle
import math
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import ndcg_score
import sys
import os

# FeatureBuilder import (ê²½ë¡œ ì¡°ì • í•„ìš”ì‹œ ìˆ˜ì •)
sys.path.append('/home/claude')  # í•„ìš”ì‹œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¡œ ìˆ˜ì •
from app.core.feature_builder import FeatureBuilder

print("ğŸš€ LightGBM Ranker v2 í•™ìŠµ ì‹œì‘")

# ===============================
# ë°ì´í„° ë¡œë“œ
# ===============================
print("\nğŸ“‚ ë°ì´í„° ë¡œë”©...")
interactions = pd.read_csv("/mnt/user-data/uploads/synthetic_interactions_nationwide.csv")
users = pd.read_csv("/mnt/user-data/uploads/synthetic_users_nationwide.csv")
meetings = pd.read_csv("/mnt/user-data/uploads/synthetic_meetings_nationwide.csv")

print(f"  - Interactions: {len(interactions):,}ê°œ")
print(f"  - Users: {len(users):,}ëª…")
print(f"  - Meetings: {len(meetings):,}ê°œ")

# rating ì—†ëŠ” ë°ì´í„° ì œê±°
interactions = interactions[interactions["rating"].notna()]
print(f"  - Valid ratings: {len(interactions):,}ê°œ")

# ===============================
# í†µê³„ í”¼ì²˜
# ===============================
print("\nğŸ“Š í†µê³„ í”¼ì²˜ ìƒì„±...")
user_stats = interactions.groupby("user_id").agg(
    user_avg_rating=("rating", "mean"),
    user_rating_std=("rating", "std"),
    user_meeting_count=("rating", "count")
).reset_index()
user_stats["user_rating_std"] = user_stats["user_rating_std"].fillna(0.3)

meeting_stats = interactions.groupby("meeting_id").agg(
    meeting_avg_rating=("rating", "mean"),
    meeting_rating_count=("rating", "count"),
    meeting_participant_count=("user_id", "count")
).reset_index()

# ===============================
# ë³‘í•©
# ===============================
print("\nğŸ”— ë°ì´í„° ë³‘í•©...")
df = interactions.merge(users, on="user_id", how="left")
df = df.merge(meetings, on="meeting_id", how="left")
df = df.merge(user_stats, on="user_id", how="left")
df = df.merge(meeting_stats, on="meeting_id", how="left")

print(f"  - ë³‘í•© í›„ í–‰ ìˆ˜: {len(df):,}ê°œ")

# ===============================
# FeatureBuilderë¡œ í”¼ì²˜ ìƒì„±
# ===============================
print("\nğŸ”§ í”¼ì²˜ ìƒì„± ì¤‘...")
fb = FeatureBuilder()
X, y, groups = [], [], []

current_user = None
group_count = 0

for idx, row in df.iterrows():
    if idx % 1000 == 0:
        print(f"  ì§„í–‰ì¤‘: {idx:,}/{len(df):,} ({idx / len(df) * 100:.1f}%)", end='\r')

    user = {
        "lat": row.get("user_lat"),
        "lng": row.get("user_lng"),
        "interests": row.get("interests"),
        "time_preference": row.get("time_preference"),
        "user_location_pref": row.get("user_location_pref"),
        "budget_type": row.get("budget_type"),
        "user_avg_rating": row.get("user_avg_rating", 3.0),
        "user_meeting_count": row.get("user_meeting_count", 0),
        "user_rating_std": row.get("user_rating_std", 0.3),
    }

    meeting = {
        "lat": row.get("meeting_lat"),
        "lng": row.get("meeting_lng"),
        "category": row.get("category"),
        "subcategory": row.get("subcategory", ""),
        "vibe": row.get("vibe"),
        "time_slot": row.get("time_slot"),
        "meeting_location_type": row.get("meeting_location_type"),
        "expected_cost": row.get("expected_cost"),
        "max_participants": row.get("max_participants"),
        "meeting_avg_rating": row.get("meeting_avg_rating", 3.0),
        "meeting_rating_count": row.get("meeting_rating_count", 0),
        "meeting_participant_count": row.get("meeting_participant_count", 0),
    }

    _, vec = fb.build(user, meeting)
    X.append(vec[0])
    y.append(row["rating"])

    if current_user != row["user_id"]:
        if current_user is not None:
            groups.append(group_count)
        current_user = row["user_id"]
        group_count = 1
    else:
        group_count += 1

groups.append(group_count)

X = np.array(X, dtype=float)
y = np.array(y, dtype=float)

print(f"\nâœ… í”¼ì²˜ ìƒì„± ì™„ë£Œ!")
print(f"  - Shape: {X.shape}")
print(f"  - Features: {len(fb.get_feature_names())}")
print(f"  - Groups: {len(groups)}")

# ===============================
# í”¼ì²˜ í†µê³„ í™•ì¸ (ì¤‘ìš”!)
# ===============================
print("\nğŸ“ˆ í”¼ì²˜ í†µê³„:")
for i, fname in enumerate(fb.get_feature_names()):
    col = X[:, i]
    print(f"  [{i:2d}] {fname:30s}: min={col.min():.3f}, max={col.max():.3f}, std={col.std():.3f}")

# âœ… ì°¨ë³„í™” ë¶€ì¡± ê²½ê³ 
zero_var_features = []
for i, fname in enumerate(fb.get_feature_names()):
    if X[:, i].std() < 0.01:
        zero_var_features.append(fname)

if zero_var_features:
    print(f"\nâš ï¸  ë¶„ì‚°ì´ ê±°ì˜ ì—†ëŠ” í”¼ì²˜ ({len(zero_var_features)}ê°œ):")
    for f in zero_var_features:
        print(f"    - {f}")

# ===============================
# Train/Test Split
# ===============================
print("\nğŸ”€ Train/Test ë¶„ë¦¬...")
split_idx = int(len(X) * 0.8)

X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

# groupsë„ ë¶„ë¦¬
train_groups = []
test_groups = []
cumsum = 0
for g in groups:
    if cumsum + g <= split_idx:
        train_groups.append(g)
    else:
        if cumsum < split_idx:
            # ê²½ê³„ì— ê±¸ì¹œ ê·¸ë£¹ ì²˜ë¦¬
            train_part = split_idx - cumsum
            if train_part > 0:
                train_groups.append(train_part)
            test_part = g - train_part
            if test_part > 0:
                test_groups.append(test_part)
        else:
            test_groups.append(g)
    cumsum += g

print(f"  - Train: {len(X_train):,} samples, {len(train_groups)} groups")
print(f"  - Test: {len(X_test):,} samples, {len(test_groups)} groups")

# ===============================
# ëª¨ë¸ í•™ìŠµ (âœ… ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°)
# ===============================
print("\nğŸ¯ LightGBM Ranker í•™ìŠµ ì‹œì‘...")

model = lgb.LGBMRanker(
    objective="lambdarank",
    metric="ndcg",

    # âœ… ë” ê°•í•œ í•™ìŠµ
    n_estimators=300,  # 200 â†’ 300
    learning_rate=0.03,  # 0.05 â†’ 0.03 (ë” ì²œì²œíˆ)

    # âœ… ë” ë³µì¡í•œ íŠ¸ë¦¬
    num_leaves=63,  # 31 â†’ 63 (ë” ì„¸ë°€í•˜ê²Œ)
    max_depth=8,  # 6 â†’ 8

    # âœ… ì •ê·œí™” ê°•í™”
    min_child_samples=20,  # ê³¼ì í•© ë°©ì§€
    subsample=0.8,  # 80% ìƒ˜í”Œë§
    colsample_bytree=0.8,  # 80% í”¼ì²˜ ìƒ˜í”Œë§
    reg_alpha=0.1,  # L1 ì •ê·œí™”
    reg_lambda=0.1,  # L2 ì •ê·œí™”

    random_state=42,
    verbose=-1
)

model.fit(
    X_train, y_train,
    group=train_groups,
    eval_set=[(X_test, y_test)],
    eval_group=[test_groups],
    eval_metric="ndcg",
    callbacks=[
        lgb.early_stopping(stopping_rounds=30, verbose=True),
        lgb.log_evaluation(period=10)
    ]
)

print("\nâœ… í•™ìŠµ ì™„ë£Œ!")

# ===============================
# ì„±ëŠ¥ í‰ê°€
# ===============================
print("\nğŸ“Š ì„±ëŠ¥ í‰ê°€...")

# Train ì˜ˆì¸¡
y_train_pred = model.predict(X_train)
print(f"\n[Train Set]")
print(f"  - Pred range: [{y_train_pred.min():.4f}, {y_train_pred.max():.4f}]")
print(f"  - Pred std: {y_train_pred.std():.4f}")
print(f"  - Unique values: {len(np.unique(np.round(y_train_pred, 2)))}")

# Test ì˜ˆì¸¡
y_test_pred = model.predict(X_test)
print(f"\n[Test Set]")
print(f"  - Pred range: [{y_test_pred.min():.4f}, {y_test_pred.max():.4f}]")
print(f"  - Pred std: {y_test_pred.std():.4f}")
print(f"  - Unique values: {len(np.unique(np.round(y_test_pred, 2)))}")

# âœ… ì°¨ë³„í™” ê²€ì‚¬
if y_test_pred.std() < 0.1:
    print("\nâš ï¸  ê²½ê³ : ì˜ˆì¸¡ê°’ì˜ ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤! ({:.4f})".format(y_test_pred.std()))
    print("    â†’ ëª¨ë¸ì´ ì œëŒ€ë¡œ ì°¨ë³„í™”í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
else:
    print(f"\nâœ… ì˜ˆì¸¡ê°’ ë¶„ì‚° ì–‘í˜¸: {y_test_pred.std():.4f}")

# ===============================
# Feature Importance
# ===============================
print("\nğŸ” Feature Importance (ìƒìœ„ 10ê°œ):")
importances = model.feature_importances_
feature_names = fb.get_feature_names()

importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)

for idx, row in importance_df.head(10).iterrows():
    print(f"  {row['feature']:30s}: {row['importance']:.1f}")

# ===============================
# ëª¨ë¸ ì €ì¥
# ===============================
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")

model_data = {
    "model": model,
    "feature_names": fb.get_feature_names(),
    "schema_version": "v2_24f_ranker_improved",
    "training_stats": {
        "n_estimators": model.n_estimators,
        "num_leaves": model.num_leaves,
        "max_depth": model.max_depth,
        "train_samples": len(X_train),
        "test_samples": len(X_test),
        "train_pred_std": float(y_train_pred.std()),
        "test_pred_std": float(y_test_pred.std()),
    }
}

output_path = "/mnt/user-data/outputs/lightgbm_ranker_v2.pkl"
with open(output_path, "wb") as f:
    pickle.dump(model_data, f)

print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_path}")
print("\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")